\chapter{Implementation}
\label{sec:implementation}

We have implemented a \prism client in about 10,000 lines of Rust code and can be found at~\cite{prismcode}. We describe the architecture of our implementation and highlight several design decisions that are key to its high performance.

\label{sec:implementation-architecture}

\section{Architecture}
% \vb{Similar to Bitcoin, a \textit{coin} is the fundamental entity of the currency in Prism. A coin is a $256$ length binary string which has a value and an owner. A \textit{transaction} `spends' a set of input coins and produces a new set of output coins such that the sum of the value of the input coins is equal to the sum of the value of the output coins. The output coins potentially have different owners from the input coins and this is the way users exchange money in the system.
% The \textit{history} (ledger) is defined as the ordered list of transactions and for a given history, \textit{UTXO} set is defined as the set of unspent output coins of all the transactions in the history. A transaction is valid w.r.t to a ledger iff it's input coins are in the UTXO set. Our implementation uses multi- input-multi-output transactions similar to pay-to-public-key (P2PK) in Bitcoin and Algorand~\cite{algorand, algorandcode}. We use Ed25519~\cite{ed25519} for digital signatures and SHA-256~\cite{sha256} as the hashing algorithm.}


Our implementation is based on the \textit{unspent transaction output (UTXO)} model, similar to that used by \bitcoin. UTXOs are generated by transactions. A transaction takes a list of UTXOs (\textit{inputs}) and defines a list of new UTXOs (\textit{outputs}). Each UTXO is only allowed to be spent once, and the {\em state} of the ledger, i.e., the state that results from applying the transactions that have been confirmed up to that point in the ledger, can be represented as a set of UTXOs. Our implementation features a simplified version of Bitcoin's scripting language, processing only pay-to-public-key (P2PK) transactions, similar to that implemented in Algorand~\cite{algorand, algorandcode}. We use Ed25519~\cite{ed25519} for cryptographic signatures and SHA-256~\cite{sha256} as the hashing algorithm.

% \gf{Are the previous two paragraphs supposed to be alternatives for each other? The first one doesn't mention scripting language at all, and the second one barely does. Perhaps we should change the subsection title.}\vb{Yes they are alternate of each other. I merged the two subsections into one}

%In such a system, each UTXO is associated with an owner, defined when the transaction is created, and identified by the hash of an Ed25519~\cite{ed25519} public key. To spend an UTXO, the owner must sign the transaction using the corresponding private key. In our implementation, we use Ed25519~\cite{ed25519} as the cryptographic signature scheme, and SHA-256~\cite{sha256} as the hashing algorithm. \ma{replace passive verbs with active verbs as much as possible} 

%We implement our client based on this transaction model.

\begin{figure}
    \centering
    \resizebox{1.0\columnwidth}{!}{\input{figures/system-diagram.tex}}
    %\input{figures/system-diagram.tex}
    \caption{\small Architecture of our \prism client implementation.}
    \label{fig:system-architecture}
\end{figure}

The system architecture is illustrated in Figure~\ref{fig:system-architecture}. Functionally it can be divided into the following three modules:

\begin{enumerate}
    \item \textit{Block Structure Manager}, which maintains the clients' view of the blockchain, and communicates with peers to exchange new blocks.
    \item \textit{Ledger Manager}, which updates the ledger based on the latest blockchain state, executes transactions, and maintains the UTXO set.
    \item \textit{Miner}, which assembles new blocks.
\end{enumerate}
%\end{CompactEnumerate}

\noindent % follow the previous paragraph
The ultimate goal of the \prism client is to maintain up-to-date information of the blockchain and the ledger. To this end, it maintains the following four data structures:

\begin{enumerate}
    \item \textit{Block Structure Database}, residing in persistent storage, stores the graph structure of the blockchain (i.e., the voter blocktrees, proposer blocktree, and transactions blocks referenced) as well as the latest confirmed order of proposer and transaction blocks.
    \item \textit{Block Database}, residing in persistent storage, stores every block a client has learned about so far. 
    \item \textit{UTXO Database}, residing in persistent storage, stores the list of all UTXOs, as well as their value and owner.
    \item \textit{Memory Pool}, residing in memory, stores the set of transactions that have not been mined in any block.
\end{enumerate}

%For the rest of this subsection, we describe these modules and data structures, and how they interact with each other. 
%We will discuss some  highlights in detail in the next subsection.


At the core of the \textbf{Block Structure Manager} are an \textit{event loop} which sends and receives network messages to/from peers, and a \textit{worker thread pool} which handles those messages. When a new block arrives, the worker thread first checks its proof of work and sortition, according to the rules specified in \S\ref{sec:mining}, and stores the new block in the Block Database.
\footnote{Checking proof of work at the earliest opportunity reduces the risk of DDoS attacks.} 
It then proceeds to relay the block to peers that have not received it. Next, the worker thread checks whether all blocks referred to by the new block, e.g. its parent, are already present in the database. If not, it buffers the block in an in-memory data structure and defers further processing until all the block's references have been received. Once a block's references have all arrived, the worker performs further validation (e.g., verifying transaction signatures), and finally, the new block is inserted into the Block Structure Database. If the block is a transaction block, the Block Structure Manager also checks the Memory Pool against the transactions included in this new block, and removes any duplicates or conflicting ones. 

\if 0
%\smallskip
At the core of the \textbf{Block Structure Manager} are an \textit{event loop} which sends and receives network messages to/from peers, and a \textit{worker thread pool} which handles those messages. In our implementation, network messages are prefixed by their length and transported over TCP. As soon as a message is received by the event loop, it is passed on to an idle worker thread, or put into a queue if none is available. There are three types of messages: \texttt{NewBlockHashes}, which advertises the hashes of blocks that a node has just mined or received from its peers. \texttt{GetBlocks}, which asks for blocks by their hashes. And \texttt{Blocks}, which contains actual blocks. 

When a new block arrives in a \texttt{Blocks} message, the worker thread first checks its proof of work and sortition, according to the rules specified in \S\ref{sec:mining}, and stores the new block in the Block Database.\footnote{Checking proof of work at the earliest opportunity reduces the risk of DDoS attacks, because an attacker would have to solve the proof-of-work puzzle to trigger potentially costly processing at other nodes.} It then proceeds to relay the block to neighbors that have not received it, following the standard procedure implemented in \bitcoin. It broadcasts a \texttt{NewBlockHashes} message to all peers to advertise this new block. Every peer that does not have this block responds with a \texttt{GetBlocks} message to request it. 

%Advertising the new block before further processing it allows us to minimize the time for a block to propagate through the network. 
Next, the worker thread checks whether all blocks referred by the new block, e.g. its parent, are already present in the databases. If not, it buffers the block in an in-memory data structure and defers further processing until all the block's references have been received. Once a block's references have all arrived, the worker performs further validation (e.g., verifying transaction signatures), and finally, the new block is inserted into the Block Structure Database. If the block is a transaction block, the Block Structure Manager also checks the Memory Pool against the transactions included in this new block, and removes any duplicates or conflicting ones. \ma{Trim the description of the block structure managerin one paragraph. We don't need the details of the messages. The important parts seem to be: event loop + workers, it relays blocks to peers similar to bitcoin after checking proof of work and sortition, it defers processing blocks if their parents are not present in db} 

\fi 

The \textbf{Ledger Manager} is a two-stage pipeline and runs asynchronously with respect to the Block Structure Manager. Its first stage, the \textit{transaction sequencer}, runs in a loop to continuously poll the Block Structure Database and try to confirm new transactions. It starts by updating the list of votes cast on each proposer block. To avoid doing wasteful work, it caches the vote counts and the tips of the voter chains, and on each invocation, it only scans through the new voter blocks. Then, it tries to confirm a leader for each level in the proposer block tree as new votes are cast, according to the rules specified in \S\ref{sec:confirmation}. In the case where a leader is selected, it queries the Block Database to retrieve the transaction blocks confirmed by the new leader, and assembles a list of confirmed transactions. The list is passed on to the second stage of the pipeline, the \textit{ledger sanitizer}. This stage maintains a pool of worker threads that executes the confirmed transactions in parallel. Specifically, a worker thread queries the UTXO Database to confirm that all inputs of the transaction are present; their owners match the signatures of the transaction;
%~\footnote{A transaction includes both the signature and the public key of the UTXO owner. Block Structure Manager has already verified the signature against the public key. Here we check that the public key matches the owner as recorded in the UTXO Database.} 
and the total value of the inputs is no less than that of the outputs. If execution succeeds, the outputs of the transaction are inserted into the UTXO Database, and the inputs are removed.

The \textbf{Miner} module assembles new blocks according to the mining procedure described in \S\ref{sec:mining}. It is implemented as a busy-spinning loop. At the start of each round, it polls the Block Structure Database and the Memory Pool to update the block it is mining. It also implements the spam mitigation mechanism described in \S\ref{sec:spamming-design}.
Like other academic implementations of PoW systems \cite{ohie,conflux}, our miner does not actually compute hashes for the proof of work, and instead simulates mining by waiting for an exponentially-distributed random delay. Solving the PoW puzzle in our experiments would waste energy for no reason, and in practice, PoW will happen primarily on dedicated hardware, e.g., application-specific integrated circuits (ASICs). So the cost of mining will not contribute to the computational bottlenecks of the consensus protocol. %Therefore, this simplification does not introduce unrealistic artifacts.

The three databases residing in the persistent storage are all built on RocksDB~\cite{rocksdb}, a high-performance key-value storage engine. We tuned the following RocksDB parameters to optimize its performance: replacing B-trees with hash tables as the index; adding bloom filters; adding a 512 MB LRU cache; and increasing the size of the write buffer to 32 MB to sustain temporary large writes. 

\if 0
% MA: if there is anything important here include it in the description of the DBs above in 1-2 sentences

The \textbf{Block Structure Database} stores different types of links between blocks as defined in the \prism protocol: voting from voter blocks to proposer blocks; referencing from proposer blocks to proposer or transaction blocks; and parental links to proposer blocks or voter blocks. It also stores various metadata, such as the leader proposer block of a level and the order of confirmed proposer blocks. 
% the level of a voter block or a proposer block; the chain number of a voter block;  
%Although those data can all be found in the original blocks, storing them in one database allows the client to quickly follow the reference links and update the ledger. 
The \textbf{Block Database} stores every block that a node has received in its original serialized format. This allows the Block Structure Manager to forward a block to peers without paying the cost of serialization every time it is requested. The \textbf{UTXO Database} stores the mapping between a UTXO and its value and owner. This database is heavily read and written, since executing every transaction requires multiple lookups (and deletions) of the inputs and insertions of the outputs. So we tuned the following RocksDB parameters to improve its performance: replacing B-trees with hash tables as the index; adding bloom filters; adding a 512 MB LRU cache; and increasing the size of the write buffer to sustain temporary large writes. 
%Besides those persistent databases, we have the \textbf{Memory Pool} implemented as an in-memory hash table.


\fi

\section{Performance Optimizations}

\label{sec:implementation-highlights}

The key challenge to implementing the \prism client is to handle its high throughput. The client must process blocks at a rate of hundreds of blocks per second, or a throughput of hundreds of Mbps, and confirm transactions at a high rate, exceeding 70,000 tps in our implementation. To handle the high throughput, our implementation exploits opportunities for parallelism in the protocol and carefully manages race conditions to achieve high concurrency. We now discuss several key performance optimizations. 

%, and we discuss in detail those design choices in this subsection.


\noindent\textbf{Asynchronous Ledger Updates.}
In traditional blockchains like \bitcoin, blocks are mined at a low rate and clients update the ledger each time they receive a new block.  However in \prism, blocks are mined at a very high rate and a only a small fraction of these blocks\,---\,those that change the proposer block leader sequence\,---\,lead to changes in the ledger. Therefore trying to update the ledger synchronously for each new block is wasteful and can become a CPU performance bottleneck. 

%update the ledger for new block bottlenecks the system

Fortunately, \prism does not require synchronous ledger updates to process blocks. Since \prism allows conflicting or duplicate transactions to appear in the ledger and performs sanitization later (\S\ref{sec:confirmation}), the client need not update the ledger for each new block. Therefore, in our implementation, the Ledger Manager runs asynchronously with respect to the Block Structure Manager, to periodically update the ledger. 
Most blockchain protocols (e.g.,  \bitcoin, \algorand, and \bng) require that miners validate a block against the current ledger prior to mining it, and therefore cannot benefit from asynchronous ledger updates. For example, in \bitcoin's current specification, when a miner mines a block $B$, it implicitly also certifies a ledger $L$ formed by tracing the blockchain from the genesis block to block $B$. 
A \bitcoin client must therefore verify that a block \textit{B} that it receives does not contain transactions conflicting with the ledger \textit{L}, and hence must update the ledger synchronously for each block. 
In principle, \bitcoin~could perform \emph{post hoc} sanitization like \prism; however, due to long block times relative to transaction verification, doing so would not improve performance.
%\ma{If I wanted to, couldn't I just add a sanitization step to Bitcoin as well, and then allow blocks to be processed asynchronously with ledger updates?}\vb{Yes. I am not sure if async will help Bitcoin.}
%\gf{Since processing isn't the bottleneck for Bitcoin, I don't think this would help. You would still need to generate blocks slowly to prevent forking.} \pv{The reviewers may have this thought too, so how about mentioning the hypothetical that Mohammad is raising and then address it premptively in the paper itself?} \gf{added a sentence/reworded--is that ok?}

\noindent\textbf{Parallel Transaction Execution.}
\label{sec:scoreboarding}
%The high rate of transactions asserts heavy pressure on the UTXO Database in particular, since executing every transaction requires multiple lookups and deletions for the inputs, and insertions for the outputs. 
Executing a transaction involves multiple reads and writes to the UTXO Database to (1) verify the validity of the input coins, (2) delete the input coins, and (3) insert the output coins. If handled sequentially, transaction execution can quickly become the bottleneck of the whole system. Our implementation therefore uses a pool of threads in the Ledger Manager to execute transactions in parallel.\footnote{Despite parallelism, the UTXO database is the bottleneck for the entire system (\S\ref{sec:eval-resource}).}
However, naively executing all transactions in parallel is problematic, because semantically the transactions in the ledger form an order, and must be executed strictly in this order to get to the correct final state (i.e., UTXO set). For example, suppose  transactions \textit{T} and \textit{T'} both use UTXO \textit{u} as input, and \textit{T} appears first in the ledger. In this case, \textit{T'} should fail, since it tries to reuse \textit{u} when it has already been spent by \textit{T}. However, if \textit{T} and \textit{T'} are executed in parallel, race condition could happen where the inputs of \textit{T'} are checked before \textit{T} deletes \textit{u} from the UTXO Database, allowing \textit{T'} to execute.

To solve this problem, we borrow the \textit{scoreboarding}~\cite{scoreboarding} technique long used in processor design. A CPU employing this method schedules multiple instructions to be executed out-of-order, if doing so will not cause conflicts such as writing to the same register. Transactions and CPU instructions are alike, in the sense that they both need to be executed in the correct order to produce correct results, only that transactions read and write UTXOs while CPU instructions read and write CPU registers. 
In the Ledger Manager, a batch of transactions are first passed through a controller thread before being dispatched to one of the idle workers in the thread pool for execution. The controller thread keeps track of the inputs and outputs of the transactions in the batch on the scoreboard (an in-memory hash table). Before scheduling a new transaction for execution, it checks that none of its inputs or outputs are present on the scoreboard. In this way, all worker threads are able to execute in parallel without any synchronization. 
%We expect that this design choice would have even greater repercussions for more complex scripting languages, since the cost of executing each transaction would increase. 

\noindent\textbf{Functional-Style Design Pattern.}
Our system must maintain shared state between several modules across both databases and in-memory data structures, creating potential for race conditions. Further, since this state is split between the memory and the database, concurrency primitives provided by RocksDB cannot solve the problem completely. For example, to update the ledger, the Ledger Manager needs to fetch the tips of the voter chains from the memory and the votes from the Block Structure Database, and they must be in sync. Locking both states with a global mutex is a straightforward solution; however, such coarse locks significantly hurt performance. 

\if 0
Our system maintains state in several in-memory data structures, e.g., the tips of each voter chain and the list of unreferred transaction blocks. All those data are shared between modules, causing potential race conditions. Furthermore, they are split between the memory and the databases, thus concurrency primitives provided by RocksDB cannot solve the problem completely. For example, to update the ledger, the Ledger Manager needs to fetch the voter chain tips from the memory and the votes from the Block Structure Database, and they must be in sync. Locking them together with a global mutex is a straightforward solution, however, such coarse locks are not scalable. 
\fi

We adopt a functional-style design pattern to define the interfaces for modules and data structures. Specifically, we abstract each module into a function that owns no shared state. Instead, state is passed explicitly between modules as inputs and outputs. For example, the functionality of the Ledger Manager can be abstracted as $\texttt{UpdateLedger}(V, V') \rightarrow \Delta T$, where $V$ and $V'$ are the previous and current voter chain tips, and $\Delta T$ are the transactions confirmed by votes between $V$ and $V'$. Then, we design the database schema to support such functions. For example, the Block Structure Database supports the query $\texttt{VoteDiff}(V, V') \rightarrow \Delta \text{Votes}$, where $\Delta \text{Votes}$ are the added and removed votes when the voter chains evolve from $V$ to $V'$. In this way, function $\texttt{UpdateLedger}$ can invoke $\texttt{VoteDiff}$ to update the votes and confirm new transactions with no need for explicit synchronization, because each function guarantees the correctness of its output with respect to its input. Functional-style design has broader benefits than enabling global-lock-free concurrency. One example is it facilitates bootstrapping (discussed in \S\ref{sec:conclusion}), where a client needs the ledger formed by leader blocks until a certain level. Another example is reverting to a previous version of the ledgers. Such queries are easily supported in our model by calling the above update ledger function. 


%supported by a client designed in functional style.
%%\gf{Could you add a bit  more detail about why this gets rid of the need for synchronization?} \ly{done}

\noindent\textbf{No Transaction Broadcasting.}
In most traditional blockchains, clients exchange pending transactions in their memory pools with peers. This incurs extra network usage, because each transaction will be broadcast twice: first as a pending transaction, and then again as part of a block. At the throughput in which \prism operates, such overhead becomes even more significant.

Our implementation does not broadcast pending transactions, because it is \textit{unnecessary} in \prism. 
In traditional blockchains like Bitcoin and Ethereum, the whole network mines a block every tens of seconds or even few minutes. 
% \gf{I would give some examples here, just to avoid someone getting mad because their favorite blockchain  does not produce blocks at that rate}\vb{Added.}
Since we cannot predict who will mine the next block, exchanging pending transactions is necessary, so that they get included in the next block regardless of who ends up mining it. In contrast, \prism generates hundreds of transaction blocks every second. This elevated block rate means that any individual miner is likely to mine a transaction block in time comparable to the delay associated with broadcasting a transaction to the rest of the network (i.e., seconds). Hence, unlike other blockchain protocols, there is little benefit for a \prism client to broadcast its transactions. Non-mining clients can transmit their transactions to one or more miners for redundancy; however, those miners do not need to relay those transactions to peers. 




%%%%%%%%%%%%%%%



\if 0

The key challenge to implementing the \prism client is to handle its high throughput. First, blocks come to a client at a rate of hundreds of blocks per second, or a throughput of hundreds of Mbps. The client needs to process those blocks and update its view of the blockchain in real time. Second, transactions are confirmed at a high rate, higher than 70,000 tps in our implementation, and the client needs to execute transactions and update the UTXO Database at such rate. Third, transaction blocks are mined at a high rate, meaning that at any moment there are lot of transaction blocks ``in-flight'' in the network. Our design solves those challenges by exploiting parallelism in the protocol, and we discuss in detail those design choices in this subsection.


\textbf{Asynchronous Ledger Updates.}
In traditional blockchains like \bitcoin, clients try to update the ledger every time a new block is received.
However, in \prism, ledger update is much more complex. It involves accounting votes from all voter chains ($m=1000$ in our experiments) and calculating proposer leader blocks based on those votes. Performing ledger update synchronously when new blocks are received will bottleneck the system, considering that hundreds of blocks arrive every second in \prism.



(Alternate: \vb{ In traditional blockchain like \bitcoin, block are mined at a low rate and each of these blocks changes in the ledger. However in \prism, blocks are mined at a very high rate and a only a small fraction of these blocks lead to changes in the ledger and therefore trying to synchronously update the ledger for new block bottlenecks the system})

As a result, in our \prism implementation, the Block Structure Manager and the Ledger Manager do not synchronize with each other. Instead, the Ledger Manager runs repeatedly in its own loop, and updates the ledger asynchronously. Note that this design choice is uniquely applicable to \prism, for the following two reasons. First, synchronous ledger update \textit{wastes CPU resource} in \prism. In protocols with simpler blockchain structures such as \bitcoin and \algorand, every new block confirms a new set of transactions in the ledger. In such cases, updating the ledger synchronously is beneficial for minimizing the latency between receiving a block and confirming corresponding transactions. This is not the case in \prism, where confirmation occurs based on the aggregate behavior of the proposer and all voter chains;\gw{Period . rather than semicolon;} Each new block individually is very unlikely to confirm any transaction, so checking it synchronously wastes CPU power. Second, synchronous ledger update \textit{is not required by the protocol}, as opposed to \bitcoin. In \bitcoin, when a miner mines on a certain block \textit{B}, it implicitly certifies its approval of a ledger \textit{L}, formed by tracing the blockchain from the genesis block to block \textit{B}. The network thus requires~\cite{bitcoin} any block mined on \textit{B} to not contain transactions conflicting with ledger \textit{L}. As a result, in a \bitcoin client, synchronous ledger update is mandatory. This is not the case for \prism, where conflicting and duplicate transactions could appear in the ledger, and sanitization happens at a later stage.



\subsection{Concurrent Access to Block Structure Database}

As described in Section~\ref{sec:implementation-architecture}, all three modules need to access the Block Structure Database concurrently, especially the Block Structure Manager which has a pool of worker threads processing new blocks in parallel. More specifically, each worker thread in the Block Structure Manager reads the database to perform block validation and insert new data; the Ledger Manager repeatedly queries the database and updates the ledger information stored in it; and the Miner repeatedly queries the database. Although such concurrent access is not forbidden by RocksDB, no consistency is guaranteed, either. For example, the Ledger Manager could have queried the database while a Block Structure Manager thread is updating it, getting inconsistent view of the blockchain. Usually a global lock on the Block Structure Database would have solved the problem, but it greatly limits concurrency.

As a solution, we build our own atomic operations using the \textit{merge operator} and the \textit{write batch} features of RocksDB. Merge operator allows us to define arbitrary atomic read-modify-write routines, e.g. incrementing a counter or appending to a list, needed by the Block Structure Manager when inserting new blocks. Merge operators are then grouped together into write batches, whose atomicity is guaranteed by RocksDB. As a result, every worker thread in the Block Structure Manager is able to update the Block Structure Database concurrently without global lock, allowing us to process new blocks efficiently in parallel. For the Ledger Manager and the Miner, we make use of the \textit{snapshot} feature. It allows creating read-only snapshots of the database, so that multiple queries can be made on a consistent view of the blockchain. \ma{Do we take a snapshot every time the ledger manager runs? Isn't this expensive?}



\subsubsection{Parallel Transaction Execution}
\label{sec:scoreboarding}
As described earlier, transaction execution in our UTXO based system involves checking that all its inputs are present in the UTXO Database, all owners of the inputs have signed the transaction, and the total value of the inputs is no less than that of the outputs, before inserting the outputs to the database. This involves multiple reads and writes to the UTXO Database. If handled sequentially, transaction execution can quickly become the bottleneck of the whole system. A natural answer to this problem is to parallelize transaction execution, and our final implementation does so by maintaining a pool of threads in the Ledger Manager for transaction execution. However, simply doing so is problematic, because semantically the transactions in the ledger form an order, and must be executed strictly in this order to get to the correct final state, or UTXO set in our case. Consider the following scenario as a simple counterexample: suppose both transaction \textit{T} and \textit{T'} use UTXO \textit{u} as input, and \textit{T} appears first in the ledger. Normally \textit{T'} should fail, since it tries to reuse \textit{u} when it has already been used by \textit{T}. However, if \textit{T} and \textit{T'} are executed in parallel, race condition could happen where the inputs of \textit{T'} are checked before \textit{T} deletes \textit{u} from the UTXO Database, leaving \textit{T'} executed without failing.

We solve this problem by borrowing the \textit{scoreboarding}~\cite{scoreboarding} technique long used in processor design. A CPU employing this method schedules multiple instructions to be executed out-of-order, if resource is available and doing so will not cause conflicts such as writing to the same register. Transactions and CPU instructions are alike, in the sense that they both need to be executed in the correct order to produce correct results, only that transactions ``read'' and ``write'' UTXOs while CPU instructions read and write CPU registers. In the Ledger Manager, all transactions are passed through a controller thread before dispatched to one of the idling workers in the thread pool. The controller thread book-keeps the inputs and outputs of all transactions in execution on the scoreboard (an in-memory hash table). Before scheduling a new transaction for execution, it checks that none of its inputs or outputs are present on the scoreboard. In this way, all worker threads are able to execute in parallel without any synchronization. We expect that this design choice would have even greater repercussions for more complex scripting languages, since the cost of executing each transaction would increase.  


\subsubsection{Concurrent Access to Block Structure Database}

\ly{functional design, talk about future benefits. the right we believe to implement this protocol. function style state formally - ``state-passing'', then we give benefits, finally give examples. 1. multiple shared states, lead to potential race conditions. not even solvable by database. 2. give one example (voter example). 3. how to reason able race conditions to avoice coarse locks. 4. explain how the functional design pattern makes you easier to reason about race conditions. (voter example again) 5. benefits are boarder, e.g. bootstrapping become easier.}

As described in Section~\ref{sec:implementation-architecture}, all three modules need to access the Block Structure Database concurrently, especially the Block Structure Manager which has a pool of worker threads processing new blocks in parallel. More specifically, each worker thread in the Block Structure Manager reads the database to perform block validation and insert new data; the Ledger Manager repeatedly queries the database and updates the ledger information stored in it; and the Miner repeatedly queries the database. Although such concurrent access is not forbidden by RocksDB, no consistency is guaranteed, either. For example, the Ledger Manager could have queried the database while a Block Structure Manager thread is updating it, getting inconsistent view of the blockchain. Usually a global lock on the Block Structure Database would have solved the problem, but it greatly limits concurrency.

As a solution, we build our own atomic operations using the \textit{merge operator} and the \textit{write batch} features of RocksDB. Merge operator allows us to define arbitrary atomic read-modify-write routines, e.g. incrementing a counter or appending to a list, needed by the Block Structure Manager when inserting new blocks. Merge operators are then grouped together into write batches, whose atomicity is guaranteed by RocksDB. As a result, every worker thread in the Block Structure Manager is able to update the Block Structure Database concurrently without global lock, allowing us to process new blocks efficiently in parallel. For the Ledger Manager and the Miner, we make use of the \textit{snapshot} feature. It allows creating read-only snapshots of the database, so that multiple queries can be made on a consistent view of the blockchain. \ma{Do we take a snapshot every time the ledger manager runs? Isn't this expensive?}

\subsubsection{Parallel Transaction Execution}
\label{sec:scoreboarding}
As described earlier, transaction execution in our UTXO based system involves checking that all its inputs are present in the UTXO Database, all owners of the inputs have signed the transaction, and the total value of the inputs is no less than that of the outputs, before inserting the outputs to the database. This involves multiple reads and writes to the UTXO Database. If handled sequentially, transaction execution can quickly become the bottleneck of the whole system. A natural answer to this problem is to parallelize transaction execution, and our final implementation does so by maintaining a pool of threads in the Ledger Manager for transaction execution. However, simply doing so is problematic, because semantically the transactions in the ledger form an order, and must be executed strictly in this order to get to the correct final state, or UTXO set in our case. Consider the following scenario as a simple counterexample: suppose both transaction \textit{T} and \textit{T'} use UTXO \textit{u} as input, and \textit{T} appears first in the ledger. Normally \textit{T'} should fail, since it tries to reuse \textit{u} when it has already been used by \textit{T}. However, if \textit{T} and \textit{T'} are executed in parallel, race condition could happen where the inputs of \textit{T'} are checked before \textit{T} deletes \textit{u} from the UTXO Database, leaving \textit{T'} executed without failing.

We solve this problem by borrowing the \textit{scoreboarding}~\cite{scoreboarding} technique long used in processor design. A CPU employing this method schedules multiple instructions to be executed out-of-order, if resource is available and doing so will not cause conflicts such as writing to the same register. Transactions and CPU instructions are alike, in the sense that they both need to be executed in the correct order to produce correct results, only that transactions ``read'' and ``write'' UTXOs while CPU instructions read and write CPU registers. In the Ledger Manager, all transactions are passed through a controller thread before dispatched to one of the idling workers in the thread pool. The controller thread book-keeps the inputs and outputs of all transactions in execution on the scoreboard (an in-memory hash table). Before scheduling a new transaction for execution, it checks that none of its inputs or outputs are present on the scoreboard. In this way, all worker threads are able to execute in parallel without any synchronization. We expect that this design choice would have even greater repercussions for more complex scripting languages, since the cost of executing each transaction would increase.  

\subsubsection{No Transaction Broadcasting}

In most traditional blockchains, clients exchange pending transactions in their Memory Pools with peers. Doing so incurs extra network usage, because each transaction will be broadcast twice: first in its own as a pending transaction, then second in a block. At the throughput where \prism operates, such overhead becomes even more significant.

Our implementation does not broadcast pending transactions at all, because it is \textit{unnecessary} for \prism. In traditional blockchains, the whole network mines a block every tens of seconds or even few minutes. Because we can not predict who will mine the next block, exchanging pending transactions becomes necessary, so that they get included in the next block regardless of who ends up mining it. In contrast, \prism generates hundreds of transaction blocks every second. This elevated block rate means that any individual miner is likely to mine a transaction block in time comparable to the delay associated with broadcasting a transaction to the rest of the network (i.e., seconds). Hence, unlike other blockchain protocols, there is little incentive for a \prism client to broadcast its transactions. In practice, non-mining clients can transmit their transactions to one or more miners; however, those miners do not need to relay those transactions for the same reason as before.

\vb{explain How do nodes reach consensus at such a high mining rate?}

\fi
